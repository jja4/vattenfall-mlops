# Incremental Data Ingestion Pipeline
#
# Runs hourly to fetch new data from Fingrid API and store in Azure Blob Storage.
# Uses DLT (data load tool) for incremental state management.
#
# Trigger: Hourly cron schedule
# 
# Required Secrets:
#   AZURE_STORAGE_CONNECTION_STRING: Azure Blob Storage connection string
#   FINGRID_API_KEY: Fingrid API authentication key

name: Ingest Data

on:
  schedule:
    # Run hourly at minute 5 (avoid exact hour to reduce API load spikes)
    - cron: '5 * * * *'
  
  # Run on push to main and pipeline branches to trigger full pipeline
  push:
    branches: [main, pipeline]
    paths:
      - 'ingestion/**'
      - 'pipeline/ingest.py'
      - '.github/workflows/ingest.yml'
  
  # Allow manual trigger
  workflow_dispatch:
    inputs:
      lookback_days:
        description: 'Initial lookback days (only for first run)'
        required: false
        default: '365'

# Prevent concurrent runs
concurrency:
  group: mlops-pipeline-${{ github.ref }}
  cancel-in-progress: false

env:
  AZURE_STORAGE_CONNECTION_STRING: ${{ secrets.AZURE_STORAGE_CONNECTION_STRING }}
  FINGRID_API_KEY: ${{ secrets.FINGRID_API_KEY }}

jobs:
  ingest:
    name: Run DLT Ingestion
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "latest"
      
      - name: Set up Python
        run: uv python install 3.11
      
      - name: Install dependencies
        run: uv sync --group training
      
      - name: Run ingestion pipeline
        run: |
          uv run python -m pipeline.ingest \
            --lookback ${{ github.event.inputs.lookback_days || '365' }}
      
      - name: Upload DLT logs on failure
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: dlt-logs
          path: |
            .dlt/
            *.log
          retention-days: 7
